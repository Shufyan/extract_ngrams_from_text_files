{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote><b>Imported all the required libraries:</b></blockquote> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords# add appropriate words that will be ignored in the analysis\n",
    "ADDITIONAL_STOPWORDS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote><b>Generic function to perform some basic cleaning (if required):</b></blockquote> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    \"\"\"\n",
    "    A simple function to clean up the data. All the words that\n",
    "    are not designated as a stop word is then lemmatized after\n",
    "    encoding and basic regex parsing are performed.\n",
    "    \"\"\"\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "      .encode('ascii', 'ignore')\n",
    "      .decode('utf-8', 'ignore')\n",
    "      .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote><b>This is the actual function to return ngrams:</b></blockquote> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(path, num):   \n",
    "\n",
    "    corpus = []\n",
    "    \n",
    "    for i in next(os.walk(path))[2]:\n",
    "        if i.endswith('.txt'):\n",
    "            f = open(os.path.join(path,i))\n",
    "            corpus.append(f.read())\n",
    "              \n",
    "    frequencies = Counter([])\n",
    "    for text in corpus:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        token = nltk.word_tokenize(text)\n",
    "        token = list(map(str.strip, token))\n",
    "        ngm = ngrams(token, num)\n",
    "        frequencies += Counter(ngm)\n",
    "    \n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote><b>Here you'll perform the actual code to call ngram function:</b></blockquote> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       freq       word1       word2          word3\n",
      "0        71         the     scarlet         letter\n",
      "1        32         the    Reverend             Mr\n",
      "2        26         out          of            the\n",
      "3        26    Reverend          Mr     Dimmesdale\n",
      "4        24         old       Roger  Chillingworth\n",
      "...     ...         ...         ...            ...\n",
      "80315     1       email  newsletter             to\n",
      "80316     1  newsletter          to           hear\n",
      "80317     1          to        hear          about\n",
      "80318     1        hear       about            new\n",
      "80319     1       about         new         eBooks\n",
      "\n",
      "[80320 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "folderPath = '<Folder_Path_for_Text_Files>'\n",
    "df = pd.DataFrame.from_dict(extract_ngrams(folderPath, n).most_common())    \n",
    "wordColNames = []\n",
    "for i in range(n):\n",
    "    wordColNames.append('word'+str(i+1))\n",
    " \n",
    "df[wordColNames] = pd.DataFrame(df[0].tolist())\n",
    "df = df.drop([0], axis=1)\n",
    "df = df.rename(columns={1:'freq'})\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
